{"cells":[{"cell_type":"markdown","source":["#Install and Import"],"metadata":{"id":"KBKp0sFvLPpX"}},{"cell_type":"code","source":["!pip install keras-unet-collection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDoL1_hQqMqR","outputId":"297a6dd2-54f8-43b1-ab82-c95bd1a2a882"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-unet-collection\n","  Downloading keras_unet_collection-0.1.13-py3-none-any.whl (67 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: keras-unet-collection\n","Successfully installed keras-unet-collection-0.1.13\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGNoFLRrh7vk"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import os\n","import random\n","from IPython.display import Image, display\n","\n","import cv2\n","from PIL import Image\n","from PIL import ImageOps\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.layers import Input, Conv2D, Dropout, Activation, UpSampling2D, GlobalMaxPooling2D, multiply\n","from tensorflow.keras.backend import max\n","from tensorflow.python.client import device_lib\n","from keras_unet_collection import models, base, utils"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ezUTXKIdiJZm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ce10f43-95c2-40cd-ae56-5ec7df496273"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#Data Preprocessing"],"metadata":{"id":"cQwOe-f0LF_q"}},{"cell_type":"markdown","source":["##Function"],"metadata":{"id":"7TblyP8SZZps"}},{"cell_type":"markdown","source":["##Label 및 Index Dic"],"metadata":{"id":"m7vjuyA3d3nr"}},{"cell_type":"code","source":["Index2Label = {0: 'Background',\n","               1: 'Head',\n","               2: 'Torso',\n","               3: 'Upper_Arms',\n","               4: 'Lower_Arms',\n","               5: 'Upper_Legs',\n","               6: 'Lower_Legs'}\n","\n","Label2Index = {'Background': 0,\n","               'Head': 1,\n","               'Torso': 2,\n","               'Upper_Arms': 3,\n","               'Lower_Arms': 4,\n","               'Upper_Legs': 5,\n","               'Lower_Legs': 6}\n","\n","# Label2Index = {v: k for k, v in Index2Label.items()}                                                                  m        m mi m3"],"metadata":{"id":"ZsuzdJV-Zbiz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 이미지 경로 생성\n"," -Input : 이미지 입력폴더\n"," -Output : 이미지 출력폴더"],"metadata":{"id":"YN7o5T6ceJns"}},{"cell_type":"code","source":["def create_images_path(folder_path):\n","    folder_path = folder_path #1\n","    imges = os.listdir(folder_path)\n","    images_path = [os.path.join(folder_path, img) for img in imges]\n","    return images_path"],"metadata":{"id":"53TqWjbkeaCr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##시각화"],"metadata":{"id":"9Lz3z1-hengZ"}},{"cell_type":"code","source":["def visualize_images(images_path, visualize=1, rotate=0):\n","    \"\"\"\n","    이미지를 시각화하는 함수\n","\n","    Args:\n","    images_path = Input\n","    #visualize = 1 = True , 2 = False\n","    #rotate = 회전각\n","\n","    Returns:\n","    images_names = Output\n","    images_array = Output<c\n","    \"\"\"\n","    # Output\n","    images_names = []\n","    images_array = []\n","\n","    for img in tqdm(images_path, desc=\"Processing images\"):\n","        image_path = img\n","        image_name = os.path.basename(image_path)\n","\n","        image = Image.open(image_path)\n","\n","        # 이미지 회전 메타데이터를 자동으로 해석하여 회전시킴\n","        image = image.rotate(rotate, expand=True)  # 0은 회전 각도, expand=True는 이미지 크기 조정 (0 / 270)\n","\n","        images_names.append(image_name)\n","        image_array = np.array(image)\n","        images_array.append(image_array)\n","\n","        # Visualize Images -------------------> Option(On/Off)\n","        if visualize:\n","\n","            print(\"Size of Image:\", image.size)\n","            print(\"Mode of Image:\", image.mode) # (RGB/L/P)\n","\n","            # print(\"Shape of Image (Numpy ):\", )\n","\n","\n","            plt.imshow(image)\n","            plt.title(f'{image_name}')\n","            plt.axis('off')  # 축 제거\n","            plt.show()\n","    else:\n","        pass\n","\n","    return images_names, images_array"],"metadata":{"id":"MaWJi-6Se2zg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_one_hot_encoded_image(one_hot_encoded_image, Index2Label=Index2Label):\n","    \"\"\"\n","    원핫인코딩된 이미지를 시각화하고 클래스별로 따로 시각화하여 보여주는 함수\n","\n","    Parameters:\n","    one_hot_encoded_image (np.ndarray): 원핫인코딩된 이미지\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    # 이미지의 형태 출력\n","    print(\"Transformed Image Shape:\", one_hot_encoded_image.shape)\n","\n","    # 전체 이미지 시각화\n","    plt.figure(figsize=(8, 8))\n","    plt.imshow(one_hot_encoded_image.argmax(axis=2), cmap='viridis', interpolation='nearest')\n","    plt.title('Original Palette Image')\n","    plt.axis('off')\n","    plt.show()\n","\n","    # 클래스별로 따로 시각화\n","    num_classes = one_hot_encoded_image.shape[2]\n","    for class_index in range(num_classes):\n","        class_image = one_hot_encoded_image[:, :, class_index]\n","        plt.figure(figsize=(8, 8))\n","        plt.imshow(class_image, cmap='gray')\n","        plt.title(f'Class: {class_index} {Index2Label[class_index]}')\n","        plt.axis('off')\n","        plt.show()"],"metadata":{"id":"6ONO0lWFfH8f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Processing"],"metadata":{"id":"T9n2fvfuepsw"}},{"cell_type":"code","source":["def palette_to_one_hot(palette_image_path, num_classes=7, imge_info=True, visualize=True):\n","    \"\"\"\n","    팔레트 이미지를 원핫인코딩하여 (2940, 1960, 7) 형태의 이미지로 변환하는 함수\n","\n","    Parameters:\n","    palette_image_path (str): 세그멘테이션된 P 모드의 PIL 팔레트 이미지 경로\n","\n","    Returns:\n","    np.ndarray: 원핫인코딩된 이미지\n","    \"\"\"\n","    # 클래스 수 정의 (배경 포함 1개)\n","    num_classes = num_classes\n","\n","    # PIL 2 Numpy\n","    palette_image = Image.open(palette_image_path)\n","    palette_image_array = np.array(palette_image)\n","\n","    # 원핫인코딩된 이미지를 저장할 배열 생성\n","    one_hot_encoded_image = np.zeros((*palette_image_array.shape, num_classes), dtype=np.uint8)\n","\n","    # 각 픽셀을 원핫인코딩된 형태로 변환\n","    for i in range(palette_image_array.shape[0]): # 2940\n","        for j in range(palette_image_array.shape[1]): # 1960\n","            pixel_value = palette_image_array[i, j]  # 현재 픽셀의 값\n","            one_hot_encoded_image[i, j, pixel_value] = 1  # 해당 클래스에 대응하는 채널에 1 할당\n","\n","    if imge_info:\n","        # 이미지 이름 출력\n","        image_name = os.path.basename(palette_image_path)\n","        print(\"Image Name:\", image_name)\n","        print(\"Original Image Shape:\", palette_image_array.shape)\n","        print(\"Transformed Image Shape:\", one_hot_encoded_image.shape)\n","        print()\n","\n","    if visualize:\n","        visualize_one_hot_encoded_image(one_hot_encoded_image,  Index2Label=Index2Label)\n","\n","    return one_hot_encoded_image"],"metadata":{"id":"G9tPKLB-e-1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_images_pkl_with_one_hot_encoding(images_paths, output_folder_path, num_classes=7, imge_info=True, visualize=False):\n","    \"\"\"\n","    이미지를 원핫인코딩하여 새로운 폴더에 저장하는 함수\n","\n","    Parameters:\n","    images_paths (list): 이미지 파일 경로의 리스트\n","    output_folder_path (str): 이미지를 저장할 폴더 이름(현재 경로에 저장)\n","    num_classes (int): segmentation 세그멘테이션 클래스 개수(배경 포함)\n","    imge_info (bool): 이미지 정보 출력 토글(default: True)\n","    visualize (bool): 이미지 시각화 토글(default: False)\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    # 새로운 폴더 생성\n","    os.makedirs(output_folder_path, exist_ok=True)\n","\n","    # one_hot_encoded_images\n","    one_hot_encoded_images = []\n","\n","    # 원핫 인코딩 수행\n","    for img_path in tqdm(images_paths, desc='Encoding images'):\n","\n","        # PIL Pallete 2 Numpy\n","        one_hot_encoded_image = palette_to_one_hot(img_path, num_classes=num_classes, imge_info=imge_info, visualize=visualize)\n","        one_hot_encoded_images.append(one_hot_encoded_image)\n","\n","    # 저장할 경로 설정\n","    save_path = os.path.join(output_folder_path, f\"one_hot_encoded_images_array.pkl\")\n","\n","    # 이미지 데이터를 피클로 저장\n","    with open(save_path, 'wb') as f:\n","        pickle.dump(one_hot_encoded_images, f)"],"metadata":{"id":"q-llYuGQfFYn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Unet"],"metadata":{"id":"9pKYUzENLBFW"}},{"cell_type":"markdown","source":["##Unet(Origin), input shape(572x572)"],"metadata":{"id":"SkjbNqfPQVoD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pU_qI7wih7vl"},"outputs":[],"source":["def conv_block(inputs, filters, kernel_size=3):\n","    x = layers.Conv2D(filters, kernel_size, activation='relu')(inputs)\n","    x = layers.Conv2D(filters, kernel_size, activation='relu')(x)\n","    return x\n","\n","def upconv_block(inputs, filters, kernel_size=2):\n","    x = layers.UpSampling2D(size=(2, 2))(inputs)\n","    x = layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(x)\n","    return x\n","\n","def unet(img_size, num_classes):\n","    inputs = keras.Input(shape=img_size + (3,))\n","\n","    # Contracting Path\n","    c1 = conv_block(inputs, 64, kernel_size=3)\n","    p1 = layers.MaxPooling2D(pool_size=(2, 2))(c1)\n","\n","    c2 = conv_block(p1, 128, kernel_size=3)\n","    p2 = layers.MaxPooling2D(pool_size=(2, 2))(c2)\n","\n","    c3 = conv_block(p2, 256, kernel_size=3)\n","    p3 = layers.MaxPooling2D(pool_size=(2, 2))(c3)\n","\n","    c4 = conv_block(p3, 512, kernel_size=3)\n","    p4 = layers.MaxPooling2D(pool_size=(2, 2))(c4)\n","\n","    # Bottom\n","    b = conv_block(p4, 1024, kernel_size=3)\n","\n","    # Expanding Path\n","    u1 = upconv_block(b, 512, kernel_size=2)\n","    c4_crop = layers.Cropping2D(cropping=((4, 4), (4, 4)))(c4)\n","    u1_concat = layers.Concatenate()([u1, c4_crop])\n","    c5 = conv_block(u1_concat, 512, kernel_size=3)\n","\n","    u2 = upconv_block(c5, 256, kernel_size=2)\n","    c3_crop = layers.Cropping2D(cropping=((16, 16), (16, 16)))(c3)\n","    u2_concat = layers.Concatenate()([u2, c3_crop])\n","    c6 = conv_block(u2_concat, 256, kernel_size=3)\n","\n","    u3 = upconv_block(c6, 128, kernel_size=2)\n","    c2_crop = layers.Cropping2D(cropping=((40, 40), (40, 40)))(c2)\n","    u3_concat = layers.Concatenate()([u3, c2_crop])\n","    c7 = conv_block(u3_concat, 128, kernel_size=3)\n","\n","    u4 = upconv_block(c7, 64, kernel_size=2)\n","    c1_crop = layers.Cropping2D(cropping=((88, 88), (88, 88)))(c1)\n","    u4_concat = layers.Concatenate()([u4, c1_crop])\n","    c8 = conv_block(u4_concat, 64, kernel_size=3)\n","\n","    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(c8)\n","\n","    return tf.keras.Model(inputs, outputs)\n","\n","keras.backend.clear_session()\n","\n","model = unet(img_size, num_classes)\n","model.summary()"]},{"cell_type":"markdown","source":["##Unet(keras_unet_collection)"],"metadata":{"id":"Iar3lWM5Qd4e"}},{"cell_type":"code","source":["keras.backend.clear_session()\n","\n","model = models.unet_2d((None, None, 3), [64, 128, 256, 512], n_labels=3,\n","                      stack_num_down=2, stack_num_up=2,\n","                      activation='ReLU', output_activation='Softmax',\n","                      batch_norm=True, pool='max', unpool='nearest', name='unet')\n","model.summary()"],"metadata":{"id":"n4mQ9MEtQyU1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Adaptive Unet"],"metadata":{"id":"Iurar582hRDj"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","def conv_block(inputs, filters, kernel_size=3):\n","    x = layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(inputs)\n","    x = layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(x)\n","    return x\n","\n","def upconv_block(inputs, filters, kernel_size=2):\n","    x = layers.UpSampling2D(size=(2, 2))(inputs)\n","    x = layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(x)\n","    return x\n","\n","def unet(img_size, num_classes):\n","    inputs = tf.keras.Input(shape=img_size + (3,))\n","\n","    # Contracting Path\n","    c1 = conv_block(inputs, 64, kernel_size=3)\n","    p1 = layers.MaxPooling2D(pool_size=(2, 2))(c1)\n","\n","    c2 = conv_block(p1, 128, kernel_size=3)\n","    p2 = layers.MaxPooling2D(pool_size=(2, 2))(c2)\n","\n","    c3 = conv_block(p2, 256, kernel_size=3)\n","    p3 = layers.MaxPooling2D(pool_size=(2, 2))(c3)\n","\n","    c4 = conv_block(p3, 512, kernel_size=3)\n","    p4 = layers.MaxPooling2D(pool_size=(2, 2))(c4)\n","\n","    # Bottom\n","    b = conv_block(p4, 1024, kernel_size=3)\n","\n","    # Expanding Path\n","    u1 = upconv_block(b, 512, kernel_size=2)\n","    u1_concat = layers.Concatenate()([u1, c4])\n","    c5 = conv_block(u1_concat, 512, kernel_size=3)\n","\n","    u2 = upconv_block(c5, 256, kernel_size=2)\n","    u2_concat = layers.Concatenate()([u2, c3])\n","    c6 = conv_block(u2_concat, 256, kernel_size=3)\n","\n","    u3 = upconv_block(c6, 128, kernel_size=2)\n","    u3_concat = layers.Concatenate()([u3, c2])\n","    c7 = conv_block(u3_concat, 128, kernel_size=3)\n","\n","    u4 = upconv_block(c7, 64, kernel_size=2)\n","    u4_concat = layers.Concatenate()([u4, c1])\n","    c8 = conv_block(u4_concat, 64, kernel_size=3)\n","\n","    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(c8)\n","\n","    return tf.keras.Model(inputs, outputs)\n"],"metadata":{"id":"ov9fRNdhhW1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining input image size and number of classes\n","img_size = (512, 512)\n","num_classes = 7"],"metadata":{"id":"M4293GBChgky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Build and Training"],"metadata":{"id":"k94OaPGVhj5m"}},{"cell_type":"code","source":["# Clearing previous sessions\n","tf.keras.backend.clear_session()"],"metadata":{"id":"5Zr2HyNOm7nj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Arguments\n","img_size = (512, 512)\n","num_classes = 7"],"metadata":{"id":"kZolyLywjYHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model = unet(img_size, num_classes)\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"nbTLO8ZLhnq6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.backend.clear_session()\n","\n","model = unet(img_size, num_classes)\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n","\n","with tf.device(\"/device:GPU:0\"):\n","    history = model.fit(train_gen, epochs=10, batch_size=batch_size, callbacks=[early_stopping_callback, metrics_callback], validation_data=val_gen, verbose=1)"],"metadata":{"id":"OtNSDVxLnfm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hist = model.fit(X, y)"],"metadata":{"id":"_rq7pSACi73c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Deeplab"],"metadata":{"id":"dH0IXzzpQl7l"}},{"cell_type":"markdown","source":["##DeeplabV3+ (BB : MobilenetV2)"],"metadata":{"id":"gyIex2cBiAux"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","\n","def deeplab_model(input_shape=(512, 512, 3), num_classes=21):\n","    # Input layer\n","    input_tensor = tf.keras.Input(shape=input_shape, name='input_image')\n","\n","    # Feature backbone (MobileNetV2 or another backbone)\n","    backbone = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet', input_tensor=input_tensor)\n","    backbone.trainable = False  # Freeze the backbone weights\n","\n","    # Atrous Spatial Pyramid Pooling (ASPP)\n","    x = backbone.output\n","    x = tf.keras.layers.Conv2D(256, (1, 1), activation='relu', padding='same', name='aspp_conv')(x)\n","    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', dilation_rate=(6, 6), name='aspp_dilated_conv1')(x)\n","    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', dilation_rate=(12, 12), name='aspp_dilated_conv2')(x)\n","    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', dilation_rate=(18, 18), name='aspp_dilated_conv3')(x)\n","    x = tf.keras.layers.Conv2D(256, (1, 1), activation='relu', padding='same', name='aspp_pooling')(x)\n","    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","    x = tf.keras.layers.Reshape((1, 1, 256))(x)\n","    x = tf.keras.layers.UpSampling2D(size=(4, 4))(x)\n","    x = tf.keras.layers.Conv2D(256, (1, 1), activation='relu', padding='same', name='aspp_upsample')(x)\n","\n","    # Skip connection from the backbone\n","    skip_connection = backbone.get_layer('block_6_expand_relu').output\n","    x = tf.keras.layers.Concatenate()([x, skip_connection])\n","\n","    # Final convolutional layers\n","    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='final_conv1')(x)\n","    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='final_conv2')(x)\n","\n","    # Output layer (1x1 convolution for segmentation)\n","    output_tensor = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax', name='output')(x)\n","\n","    # Create the model\n","    model = Model(inputs=input_tensor, outputs=output_tensor)\n","\n","    return model\n","\n","# Instantiate the model\n","model = deeplab_model()\n","\n","# Display the model summary\n","model.summary()"],"metadata":{"id":"c2iJ5PJCiTt7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Arguments"],"metadata":{"id":"-p63qubpjC4U"}},{"cell_type":"markdown","source":["##Training"],"metadata":{"id":"MAmljkGJjGlg"}},{"cell_type":"code","source":["hist = model.fit(X, y)"],"metadata":{"id":"L0HLtIBbjIvX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Callback"],"metadata":{"id":"kAfW7mfCK7iY"}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","early_stopping_callback = EarlyStopping(patience=3, monitor='val_loss')"],"metadata":{"id":"wn-cAn_3f7-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCEdJrtEh7vl"},"outputs":[],"source":["from keras.callbacks import Callback\n","\n","class MetricsCallback(Callback):\n","    def __init__(self, val_gen):\n","        self.val_gen = val_gen\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        pixel_accuracy, mean_accuracy, mean_iou = evaluate(model, criterion, self.val_gen, device, num_classes=7)\n","        print(f'Epoch [{epoch + 1}], Validation Pixel Accuracy: {pixel_accuracy:.4f}')\n","        print(f'Epoch [{epoch + 1}], Validation Mean Accuracy: {mean_accuracy:.4f}')\n","        print(f'Epoch [{epoch + 1}], Validation mIoU: {mean_iou:.4f}')\n","\n","metrics_callback = MetricsCallback(val_gen)"]},{"cell_type":"markdown","source":["#Save!!!!!!"],"metadata":{"id":"byoJhXqKKrZC"}},{"cell_type":"code","source":["model.save('Unet.h5')"],"metadata":{"id":"B76lqAFUa6Mg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Metric"],"metadata":{"id":"KlvmaRszJmv7"}},{"cell_type":"markdown","source":["##1.iou"],"metadata":{"id":"9Bt4IfxCG7h1"}},{"cell_type":"code","source":["def iou(y_true, y_pred, dtype=tf.float32):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","\n","    area_true = tf.reduce_sum(y_true_f)\n","    area_pred = tf.reduce_sum(y_pred_f)\n","    union = area_true + area_pred - intersection\n","\n","    iou = tf.where(tf.equal(union, 0), 0.0, tf.math.divide(intersection, union))\n","    return iou.numpy()\n","\n","def dice_coef(y_true, y_pred, smooth=1e-6):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    return dice.numpy()"],"metadata":{"id":"mjNz8Y04GTeC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.Dice coef"],"metadata":{"id":"JbrrLobzJsjA"}},{"cell_type":"code","source":["def dice_coef(y_true, y_pred, smooth=1e-6):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    return dice.numpy()"],"metadata":{"id":"5Bt4jNUiJtng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Loss"],"metadata":{"id":"4Cjs0ArSG43z"}},{"cell_type":"markdown","source":["##1. iou loss"],"metadata":{"id":"Zqsm4lALJ0Pt"}},{"cell_type":"code","source":["#loss\n","def iou_loss(y_true, y_pred, smooth=1e-6):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","\n","    area_true = tf.reduce_sum(y_true_f)\n","    area_pred = tf.reduce_sum(y_pred_f)\n","    union = area_true + area_pred - intersection\n","\n","    iou = tf.where(tf.equal(union, 0), 0.0, tf.math.divide(intersection, union))\n","\n","    return 1 - iou.numpy()"],"metadata":{"id":"aQXn49Q-KMGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2. dice loss\n","\n","$$diceloss = 1 - dicecoef$$"],"metadata":{"id":"0g9so_AbJ27E"}},{"cell_type":"code","source":["def dice_loss(y_true, y_pred, smooth=1e-6):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    return 1 - dice"],"metadata":{"id":"ydeVYauQKTad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3. dice BCE loss\n","\n","$$ dice BCE loss = BCE + diceloss$$"],"metadata":{"id":"8s1BolcZKBAG"}},{"cell_type":"code","source":["def dice_BCE_loss(y_true, y_pred, smooth=1e-6):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    dice_loss = 1 - (2*intersection + smooth) / (K.sum(y_true) + K.sum(y_pred) + smooth)\n","\n","    BCE =  losses.binary_crossentropy(y_true_f, y_pred_f)\n","\n","    Dice_BCE_loss = BCE + dice_loss\n","\n","    return Dice_BCE_loss.numpy()\n"],"metadata":{"id":"7ZKTJ_1yKRyw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##4. focal loss\n","- 2017년, 극도로 불균형한 데이터 세트를 해결하기 위한 수단으로 도입\n","$$ Focal loss = BCE*(1-pred)^r$$"],"metadata":{"id":"9gBUlawAKDhk"}},{"cell_type":"code","source":["alpha = 0.8\n","gamma = 2\n","\n","def focal_loss(targets, inputs, alpha=alpha, gamma=gamma):\n","    inputs = K.flatten(inputs)\n","    targets = K.flatten(targets)\n","\n","    bce = K.binary_crossentropy(targets, inputs)\n","    bce_exp = K.exp(-bce)\n","    focal_loss = K.mean(alpha * K.pow((1-bce_exp), gamma) * bce)\n","\n","    return focal_loss\n"],"metadata":{"id":"Js9pnZHyKaJ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##5. Tversky loss\n","- 서로 다른 유형의 오류가 얼마나 심하게 처벌되는지를 조정할 수 있는 상수를 활용하여 불균형 데이터셋에 대해 최적화하도록 설계\n","$$ 1 - TP/(TP + a*FP + b*FN) $$\n"],"metadata":{"id":"7NqPMG0jKGU8"}},{"cell_type":"code","source":["alpha = 0.5\n","beta = 0.5\n","\n","def tversky_loss(targets, inputs, alpha=alpha, beta=beta, smooth=1e-6):\n","        inputs = K.flatten(inputs)\n","        targets = K.flatten(targets)\n","\n","        tp = K.sum((inputs * targets))\n","        fp = K.sum(((1-targets) * inputs))\n","        fn = K.sum((targets * (1-inputs)))\n","\n","        tversky = (tp + smooth)/(tp + alpha*fp + beta*fn + smooth)\n","\n","        return 1 - tversky"],"metadata":{"id":"NuUGorucKc6t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###출처:  \n","https://github.com/yingkaisha/keras-unet-collection/tree/d30f14a259656d2f26ea11ed978255d6a7d0ce37\n","\n","https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook\n","  \n","https://github.com/zhengyang-wang/3D-Unet--Tensorflow/blob/c5d603a69243a69dd6d89edefdf1ba249640450b/utils/HausdorffDistance.py#L8"],"metadata":{"id":"KvBV64zBIQFD"}},{"cell_type":"markdown","source":["#GPU(CUDA)"],"metadata":{"id":"Zm138YcFKkeC"}},{"cell_type":"code","source":["for dev in device_lib.list_local_devices(): # 사용가능한 device(CPU, GPU 등) 목록\n","    print(dev.device_type, dev.memory_limit)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"ARON74aoq9ov","outputId":"cf0460f1-300b-4bca-e76b-85ffc46cc62b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'device_lib' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8a64d8f85cb2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 사용가능한 device(CPU, GPU 등) 목록\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'device_lib' is not defined"]}]},{"cell_type":"code","source":["!pip install numba\n","\n","from numba import cuda\n","\n","dev = cuda.get_current_device(); dev.reset()"],"metadata":{"id":"ENHwpaoHHc39"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","collapsed_sections":["KBKp0sFvLPpX","dacUvnNGeCO4","SkjbNqfPQVoD","Iar3lWM5Qd4e","dH0IXzzpQl7l","kAfW7mfCK7iY","KlvmaRszJmv7","9Bt4IfxCG7h1","JbrrLobzJsjA","4Cjs0ArSG43z","Zm138YcFKkeC"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}